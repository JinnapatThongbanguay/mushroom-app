# streamlit_app.py
# üçÑ Mushroom Safety Finder (RAG + CLIP + FAISS) + Gemini (on-demand)
# Safe Mode ON ‚Äî Minimal UI, User Prompts, Ready for Streamlit Cloud deploy

import os
import streamlit as st
from PIL import Image
import numpy as np
import pickle
import faiss
import torch
from transformers import CLIPProcessor, CLIPModel
import google.generativeai as genai
from typing import Tuple, List, Dict, Any

# CONFIG
st.set_page_config(page_title="Mushroom Safety Finder (RAG + Gemini)",
                   page_icon="üçÑ", layout="centered")

KB_PATH = "mushroom_knowledge_base.pkl"   # created by build_kb.py
TOP_K = 5
CONFIDENCE_HIGH = 0.75
CONFIDENCE_MEDIUM = 0.55
SAFE_CONF_THRESH = 0.60   # threshold for forcing "do not eat" when low confidence

# Load CLIP model & processor
# cached for streamlit
@st.cache_resource
def load_clip():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    return model, processor, device

clip_model, clip_processor, device = load_clip()

# Load KB and build FAISS index
@st.cache_resource
def load_kb_and_index(kb_path: str = KB_PATH):
    if not os.path.exists(kb_path):
        return None, None, None
    with open(kb_path, "rb") as f:
        kb = pickle.load(f)

    all_feats = []
    metadata = []
    for species, data in kb.items():
        feats = data.get("features")
        if feats is None:
            continue
        # ensure float32
        for i, feat in enumerate(feats):
            all_feats.append(np.asarray(feat, dtype="float32"))
            metadata.append((species, data.get("label", "Unknown"), i, data.get("paths", [])))

    if not all_feats:
        return kb, None, metadata

    all_feats = np.stack(all_feats)
    dim = all_feats.shape[1]
    index = faiss.IndexFlatIP(dim)
    # assume features are normalized in build_kb
    index.add(all_feats)
    return kb, index, metadata

kb, faiss_index, metadata = load_kb_and_index(KB_PATH)

# Setup Gemini (on-demand)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") or (st.secrets["GEMINI_API_KEY"] if "GEMINI_API_KEY" in st.secrets else None)
gemini_model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        # use a supported model
        MODEL_NAME = "models/gemini-2.5-flash"
        gemini_model = genai.GenerativeModel(MODEL_NAME)
    except Exception as e:
        gemini_model = None
        st.warning(f"Gemini init warning: {e}")

# Helpers: feature extraction, retrieval, prediction
def extract_image_features_pil(img: Image.Image) -> np.ndarray:
    img = img.convert("RGB")
    inputs = clip_processor(images=img, return_tensors="pt").to(device)
    with torch.no_grad():
        feats = clip_model.get_image_features(**inputs)
        feats = feats / feats.norm(dim=-1, keepdim=True)
    return feats.cpu().numpy()[0]

def retrieve_similar_examples(query_feat: np.ndarray, top_k: int = TOP_K) -> List[Dict[str,Any]]:
    if faiss_index is None:
        return []
    q = query_feat.reshape(1, -1).astype("float32")
    sims, idxs = faiss_index.search(q, top_k)
    results = []
    for sim, idx in zip(sims[0], idxs[0]):
        if idx < 0 or idx >= len(metadata):
            continue
        species, label, img_idx, paths = metadata[idx]
        results.append({"species": species, "label": label, "similarity": float(sim), "image_idx": img_idx, "paths": paths})
    return results

def predict_rag_from_image(img: Image.Image) -> Tuple[str, float, str, List[Dict[str,Any]]]:
    try:
        qfeat = extract_image_features_pil(img)
    except Exception as e:
        return "Unknown", 0.0, "unknown", []
    retrieved = retrieve_similar_examples(qfeat, top_k=TOP_K)
    label_scores = {"Edible": 0.0, "Poisonous": 0.0}
    for r in retrieved:
        lab = r.get("label", "Unknown")
        if lab in label_scores:
            label_scores[lab] += r.get("similarity", 0.0)
    total = sum(label_scores.values())
    if total <= 0:
        probs = {"Edible": 0.5, "Poisonous": 0.5}
    else:
        probs = {k: v/total for k, v in label_scores.items()}
    predicted = max(probs, key=probs.get)
    confidence = float(probs[predicted])
    top_species = retrieved[0]["species"] if retrieved else "unknown"
    return predicted, confidence, top_species, retrieved

# Safe Mode decision
def safe_decision(predicted_label: str, confidence: float, species_key: str) -> Dict[str,Any]:
    has_info = (kb is not None and species_key in kb)
    if predicted_label == "Poisonous":
        return {"risk": "‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á (‡πÄ‡∏´‡πá‡∏î‡∏û‡∏¥‡∏©)", "advice": "‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î ‡πÅ‡∏•‡∏∞‡∏£‡∏µ‡∏ö‡πÑ‡∏õ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ", "safe_to_eat": False}
    if predicted_label == "Edible" and not has_info:
        return {"risk":"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏î‡πâ", "advice":"‡πÅ‡∏°‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡πá‡∏î‡∏Å‡∏¥‡∏ô‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô ‚Üí ‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î", "safe_to_eat": False}
    if predicted_label == "Edible" and has_info and confidence >= CONFIDENCE_HIGH:
        return {"risk":"‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≥ (‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô)", "advice":"‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏∏‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô", "safe_to_eat": True}
    if predicted_label == "Edible" and has_info and confidence >= CONFIDENCE_MEDIUM:
        return {"risk":"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "advice":"‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô", "safe_to_eat": False}
    # fallback
    return {"risk":"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏ô‡∏¥‡∏î‡πÑ‡∏î‡πâ", "advice":"‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î", "safe_to_eat": False}


# Gemini prompt builder (User / Expert modes)
def build_gemini_prompt(species_key: str, predicted_label: str, confidence: float, expert_mode: bool = False) -> str:
    if expert_mode:
        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏´‡πá‡∏î (Mycologist)

‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:
- ‡∏ä‡∏ô‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏î: {species_key}
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: {predicted_label}
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {confidence:.2%}

‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏ô‡∏¥‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ (‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏î ‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"):
- scientific_name
- thai_name
- edibility
- toxicity_level
- physical_characteristics (‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏û‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á)
- habitat
- symptoms
- first_aid
- warning

‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î:
- ‡∏´‡∏≤‡∏Å confidence < {SAFE_CONF_THRESH*100:.0f}% ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤ "‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô" ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡πç‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏¥‡∏ô
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON
"""
    else:
        # ‡πÇ‡∏´‡∏°‡∏î‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢/‡∏Ñ‡∏≥‡∏™‡∏£‡∏∏‡∏õ)
        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏´‡πá‡∏î (Mycologist)

‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:
- ‡∏ä‡∏ô‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏î: {species_key}
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: {predicted_label}
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {confidence:.2%}

‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡πá‡∏î‡∏ä‡∏ô‡∏¥‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏µ:
- ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡πá‡∏î
- ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏î‡πà‡∏ô (‡∏™‡∏±‡πâ‡∏ô ‡πÜ)
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
- ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô (‡∏™‡∏±‡πâ‡∏ô)
- ‡∏Å‡∏≤‡∏£‡∏õ‡∏ê‡∏°‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• (‡∏™‡∏±‡πâ‡∏ô)
- ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢

‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î:
- ‡∏´‡πâ‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞" ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á
- ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô "‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"
- ‡∏´‡∏≤‡∏Å confidence < {SAFE_CONF_THRESH*100:.0f}% ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤ '‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô' ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏¥‡∏ô
- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
"""
    return prompt.strip()

def ask_gemini_text(species_key: str, predicted_label: str, confidence: float, expert_mode: bool=False) -> str:
    if gemini_model is None:
        return "Gemini API not configured. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡πÉ‡∏ô environment ‡∏´‡∏£‡∏∑‡∏≠ Streamlit secrets ‡∏Å‡πà‡∏≠‡∏ô"
    prompt = build_gemini_prompt(species_key, predicted_label, confidence, expert_mode)
    try:
        resp = gemini_model.generate_content(prompt)
        # return textual content only
        return resp.text
    except Exception as e:
        return f"Gemini error: {e}"

# UI
st.title("Mushroom Safety Finder (RAG + Gemini)")
st.write("‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏´‡πá‡∏î‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û (CLIP + FAISS + Knowledge Base) ‚Äî Safe Mode ON")
st.info("‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á")

uploaded = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡πÄ‡∏´‡πá‡∏î (jpg, png, heic)", type=["jpg","jpeg","png", "heic"])
# expert_mode ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô False ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ sidebar ‡πÅ‡∏ó‡∏ô
expert_mode = False # ‡πÉ‡∏ä‡πâ User Mode ‡πÄ‡∏™‡∏°‡∏≠

if uploaded:
    try:
        image = Image.open(uploaded).convert("RGB")
        st.image(image, use_container_width=True)
    except Exception:
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        st.stop()

    if kb is None or faiss_index is None:
        st.warning("‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ (mushroom_knowledge_base.pkl) ‡∏´‡∏£‡∏∑‡∏≠ FAISS index ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° ‚Äî ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô build_kb.py ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö")

    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (RAG)..."):
        predicted_label, confidence, top_species, retrieved = predict_rag_from_image(image)

    decision = safe_decision(predicted_label, confidence, top_species)

    st.markdown("### ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢")
    st.write(f"- **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à (model):** {confidence*100:.2f}%")
    st.write(f"- **‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á:** {decision['risk']}")
    st.write(f"- **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** {decision['advice']}")

    st.markdown("#### ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (On-demand)")
    st.write("‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á / ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç ‡πÉ‡∏´‡πâ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á")

    if st.button("‡∏Ç‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å AI (Gemini)"):
        with st.spinner("Gemini ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢..."):
            # ‡πÉ‡∏ä‡πâ expert_mode ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ (‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ False)
            gemini_text = ask_gemini_text(top_species, predicted_label, confidence, expert_mode=expert_mode)
        st.markdown("#### ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏à‡∏≤‡∏Å AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢")
        st.write(gemini_text)
        

else:
    st.caption("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‚Äî ‡∏ñ‡πà‡∏≤‡∏¢‡∏°‡∏∏‡∏°‡∏ö‡∏ô, ‡πÉ‡∏ï‡πâ‡∏î‡∏≠‡∏Å, ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏ô‡∏Å‡πâ‡∏≤‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô")

# footer note about gemini
if gemini_model is None:
    st.caption("Gemini: Not configured. ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡πÉ‡∏ô environment ‡∏´‡∏£‡∏∑‡∏≠ Streamlit secrets.")
else:
    st.caption("Gemini: Enabled (on-demand). API calls made only when user requests details.")
